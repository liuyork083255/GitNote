使用参数说明：
--where 'id<3000'	设置操作条件

--limit 10000	每次取1000行数据给pt-archive处理

--txn-size 1000	设置1000行为一个事务提交一次

--progress 5000	每处理5000行输出一次处理信息

--statistics	结束的时候给出统计信息：开始的时间点，结束的时间点，查询的行数，归档的行	
				数，删除的行数，以及各个阶段消耗的总的时间和比例，便于以此进行优化。只要不加
				上--quiet，默认情况下pt-archive都会输出执行过程的
				
--charset=UTF8	指定字符集为UTF8

--no-delete	表示不删除原来的数据，注意：如果不指定此参数，所有处理完成后，都会清理原表
				中的数据
				
--bulk-delete	批量删除source上的旧数据

--bulk-insert	批量插入数据到dest主机 (看dest的general log发现它是通过在dest主机上LOAD DATA 
				LOCAL INFILE插入数据的)
				
--purge		删除source数据库的相关匹配记录

--local		不把optimize或analyze操作写入到binlog里面（防止造成主从延迟巨大）

--analyze=ds	操作结束后，优化表空间（d表示dest，s表示source）
				默认情况下，pt-archiver操作结束后，不会对source、dest表执行analyze或optimize操
				作，因为	这种操作费时间，并且需要你提前预估有足够的磁盘空间用于拷贝表。一般
				建议也是pt-archiver操作结束后，在业务低谷手动执行analyze table用以回收表空间
				
--replace		将insert into 语句改成replace写入到dest库

--sleep 120 	每次归档了limit个行记录后的休眠120秒（单位为秒）

--file '/root/test.txt' 生成文件

--analyze=ds      操作结束后，优化表空间（d表示dest，s表示source）
				默认情况下，pt-archiver操作结束后，不会对source、dest表执行analyze或
				optimize操作，因为这种操作费时间，并且需要你提前预估有足够的磁盘空间用于
				拷贝表。一般建议也是pt-archiver操作结束后，在业务低谷手动执行analyze table
				用以回收表空间。
				
--dry-run		试运行,可以打印出查询计划,看看是否为最坏的查询-全表扫描.

注意：Specify at least one of "--dest","--file", or "--purge".

pt-archiver执行指定的文件：
	创建一个文件名为xxx.conf
		source = h=127.0.0.1,P=3306,u=root,p=123456,D=liu_yang,t=bond_best_quote_webbond_fmt
		dest = h=127.0.0.1,P=3306,u=root,p=123456,D=liu_yang,t=bond_bcakup
		file = /root/pt_test.sql
		progress = 10
		limit = 10000
		txn-size = 10000
		where = 1=1
		charset = utf8
		statistics
		no-delete
		
	调用：pt-archiver --confing /xxx.conf
	注意：key如果有value，则用=分割，没有value则直接写key就好了

	
复杂功能：
	1 导出指定的列
		配置文件中加入：columns = autoId,quoteMonth,bondKey 即可
		后面的列用用逗号分割
		注意: 没有指定列,在原表也会被删除.也就说,未选择列的数据,就会丢失
		但是如果加了no-delete就不会被删除
	2 插入目标表中，是会根据导出的列明按名字对应注入的，也就是说如果原表10个列，
		现在导出5列，目标表也是五列，那么会自动对应插入的，顺序即使乱序也没有关系
	3 在where里面是可以使用mysql的函数来处理复杂的删选条件













